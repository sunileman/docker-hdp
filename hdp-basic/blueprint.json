{
    "Blueprints" :
        {
            "blueprint_name" : "hdp-basic",
            "stack_name" : "HDP",
            "stack_version" : "2.4"
        },
    "configurations": [
        {
            "core-site": {
                "properties_attributes" : { },
                "properties" : {
                    "hadoop.proxyuser.root.groups": "*",
                    "hadoop.proxyuser.root.hosts": "*",
                    "hadoop.proxyuser.hcat.groups": "*",
                    "hadoop.proxyuser.hcat.hosts": "*"
                }
            }
        },{
            "hdfs-site": {
                "properties_attributes" : { },
                "properties" : {
                    "dfs.replication": "1"
                }
            }
        },{
            "webhcat-site": {
                "properties_attributes" : { },
                "properties" : {
                    "webhcat.proxyuser.root.groups": "*",
                    "webhcat.proxyuser.root.hosts": "*"
                }
            }
        },{
            "spark-defaults": {
                "properties_attributes" : { },
                "properties": {
                    "spark.driver.extraJavaOptions": "",
                    "spark.history.kerberos.keytab": "none",
                    "spark.history.kerberos.principal": "none",
                    "spark.history.provider": "org.apache.spark.deploy.yarn.history.YarnHistoryProvider",
                    "spark.history.ui.port": "18080",
                    "spark.yarn.am.extraJavaOptions": "",
                    "spark.yarn.applicationMaster.waitTries": "10",
                    "spark.yarn.containerLauncherMaxThreads": "25",
                    "spark.yarn.driver.memoryOverhead": "384",
                    "spark.yarn.executor.memoryOverhead": "384",
                    "spark.yarn.max.executor.failures": "2",
                    "spark.yarn.preserve.staging.files": "false",
                    "spark.yarn.queue": "default",
                    "spark.yarn.scheduler.heartbeat.interval-ms": "5000",
                    "spark.yarn.submit.file.replication": "1"
                }
            }
        },{
            "spark-env": {
                "properties_attributes" : { },
                "properties": {
                    "content": "\n#!/usr/bin/env bash\n\n# This file is sourced when running various Spark programs.\n# Copy it as spark-env.sh and edit that to configure Spark for your site.\n\n# Options read in YARN client mode\n#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\n#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\n#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n#SPARK_DRIVER_MEMORY=\"512 Mb\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n#SPARK_YARN_APP_NAME=\"spark\" #The name of your application (Default: Spark)\n#SPARK_YARN_QUEUE=\"~@~Xdefault~@~Y\" #The hadoop queue to use for allocation requests (Default: @~Xdefault~@~Y)\n#SPARK_YARN_DIST_FILES=\"\" #Comma separated list of files to be distributed with the job.\n#SPARK_YARN_DIST_ARCHIVES=\"\" #Comma separated list of archives to be distributed with the job.\n\n# Generic options for the daemons used in the standalone deploy mode\n\n# Alternate conf dir. (Default: ${SPARK_HOME}/conf)\nexport SPARK_CONF_DIR=${SPARK_HOME:-{{spark_home}}}/conf\n\n# Where log files are stored.(Default:${SPARK_HOME}/logs)\n#export SPARK_LOG_DIR=${SPARK_HOME:-{{spark_home}}}/logs\nexport SPARK_LOG_DIR={{spark_log_dir}}\n\n# Where the pid file is stored. (Default: /tmp)\nexport SPARK_PID_DIR={{spark_pid_dir}}\n\n# A string representing this instance of spark.(Default: $USER)\nSPARK_IDENT_STRING=$USER\n\n# The scheduling priority for daemons. (Default: 0)\nSPARK_NICENESS=0\n\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\n\n# The java implementation to use.\nexport JAVA_HOME={{java_home}}\n\nif [ -d \"/etc/tez/conf/\" ]; then\n  export TEZ_CONF_DIR=/etc/tez/conf\nelse\n  export TEZ_CONF_DIR=\nfi",
                    "spark_group": "spark",
                    "spark_log_dir": "/var/log/spark",
                    "spark_pid_dir": "/var/run/spark",
                    "spark_user": "spark"
                }
            }
        }
    ],
    "host_groups" : [
        {
        "name" : "host_group_1",
        "cardinality" : "1",
            "components" : [
                { "name": "NAMENODE" },
		{ "name": "SECONDARY_NAMENODE" },
                { "name": "METRICS_COLLECTOR" },
                { "name": "APP_TIMELINE_SERVER" },
                { "name": "SPARK_JOBHISTORYSERVER" },
                { "name": "DATANODE" },
                { "name": "NODEMANAGER" },
                { "name": "METRICS_MONITOR" },
                { "name": "HDFS_CLIENT" },
                { "name": "SPARK_CLIENT" },
                { "name": "YARN_CLIENT" },
                { "name": "RESOURCEMANAGER" },
                { "name": "HISTORYSERVER" }
            ]
        }
    ]
}
